{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic statistical testing\n",
    "\n",
    "En esta conferencia vamos a revisar basics of statistical testing in python. Hablaremos sobre hypothesis testing, statistical significance y usaremos scipy to run student's t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En data science usamos statistics en un monton de caminos diferentes. Vamos a refrescar nuestro conocimiento\n",
    "# sobre que es hypothesis testing, lo cual es a core data analysis activity behind experimentation.\n",
    "# El objetivo de una hypothesis testing es determinar si, por ejemplo, las dos diferentes condiciones que tenemos\n",
    "# en un experimento have resulted in different impacts\n",
    "\n",
    "# importemos nuestras librerias usuales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ahora traigams algo nuevo. Traeremos algunas librerias nuevas de scipy\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scipy es una interesante coleccion de librerias para data science y hemos usado muchas de estas librerias.\n",
    "# Scipy incluye pandas y numpy pero tambien tiene plotting libraries como matplotlib, y number of \n",
    "# scientific library functions as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>assignment1_grade</th>\n",
       "      <th>assignment1_submission</th>\n",
       "      <th>assignment2_grade</th>\n",
       "      <th>assignment2_submission</th>\n",
       "      <th>assignment3_grade</th>\n",
       "      <th>assignment3_submission</th>\n",
       "      <th>assignment4_grade</th>\n",
       "      <th>assignment4_submission</th>\n",
       "      <th>assignment5_grade</th>\n",
       "      <th>assignment5_submission</th>\n",
       "      <th>assignment6_grade</th>\n",
       "      <th>assignment6_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B73F2C11-70F0-E37D-8B10-1D20AFED50B1</td>\n",
       "      <td>92.733946</td>\n",
       "      <td>2015-11-02 06:55:34.282000000</td>\n",
       "      <td>83.030552</td>\n",
       "      <td>2015-11-09 02:22:58.938000000</td>\n",
       "      <td>67.164441</td>\n",
       "      <td>2015-11-12 08:58:33.998000000</td>\n",
       "      <td>53.011553</td>\n",
       "      <td>2015-11-16 01:21:24.663000000</td>\n",
       "      <td>47.710398</td>\n",
       "      <td>2015-11-20 13:24:59.692000000</td>\n",
       "      <td>38.168318</td>\n",
       "      <td>2015-11-22 18:31:15.934000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1</td>\n",
       "      <td>86.790821</td>\n",
       "      <td>2015-11-29 14:57:44.429000000</td>\n",
       "      <td>86.290821</td>\n",
       "      <td>2015-12-06 17:41:18.449000000</td>\n",
       "      <td>69.772657</td>\n",
       "      <td>2015-12-10 08:54:55.904000000</td>\n",
       "      <td>55.098125</td>\n",
       "      <td>2015-12-13 17:32:30.941000000</td>\n",
       "      <td>49.588313</td>\n",
       "      <td>2015-12-19 23:26:39.285000000</td>\n",
       "      <td>44.629482</td>\n",
       "      <td>2015-12-21 17:07:24.275000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0F62040-CEB0-904C-F563-2F8620916C4E</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 05:36:02.389000000</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 06:39:44.416000000</td>\n",
       "      <td>68.410033</td>\n",
       "      <td>2016-01-15 20:22:45.882000000</td>\n",
       "      <td>54.728026</td>\n",
       "      <td>2016-01-11 12:41:50.749000000</td>\n",
       "      <td>49.255224</td>\n",
       "      <td>2016-01-11 17:31:12.489000000</td>\n",
       "      <td>44.329701</td>\n",
       "      <td>2016-01-17 16:24:42.765000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FFDF2B2C-F514-EF7F-6538-A6A53518E9DC</td>\n",
       "      <td>86.030665</td>\n",
       "      <td>2016-04-30 06:50:39.801000000</td>\n",
       "      <td>68.824532</td>\n",
       "      <td>2016-04-30 17:20:38.727000000</td>\n",
       "      <td>61.942079</td>\n",
       "      <td>2016-05-12 07:47:16.326000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-07 16:09:20.485000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-24 12:51:18.016000000</td>\n",
       "      <td>44.598297</td>\n",
       "      <td>2016-05-26 08:09:12.058000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5ECBEEB6-F1CE-80AE-3164-E45E99473FB4</td>\n",
       "      <td>64.813800</td>\n",
       "      <td>2015-12-13 17:06:10.750000000</td>\n",
       "      <td>51.491040</td>\n",
       "      <td>2015-12-14 12:25:12.056000000</td>\n",
       "      <td>41.932832</td>\n",
       "      <td>2015-12-29 14:25:22.594000000</td>\n",
       "      <td>36.929549</td>\n",
       "      <td>2015-12-28 01:29:55.901000000</td>\n",
       "      <td>33.236594</td>\n",
       "      <td>2015-12-29 14:46:06.628000000</td>\n",
       "      <td>33.236594</td>\n",
       "      <td>2016-01-05 01:06:59.546000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             student_id  assignment1_grade  \\\n",
       "0  B73F2C11-70F0-E37D-8B10-1D20AFED50B1          92.733946   \n",
       "1  98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1          86.790821   \n",
       "2  D0F62040-CEB0-904C-F563-2F8620916C4E          85.512541   \n",
       "3  FFDF2B2C-F514-EF7F-6538-A6A53518E9DC          86.030665   \n",
       "4  5ECBEEB6-F1CE-80AE-3164-E45E99473FB4          64.813800   \n",
       "\n",
       "          assignment1_submission  assignment2_grade  \\\n",
       "0  2015-11-02 06:55:34.282000000          83.030552   \n",
       "1  2015-11-29 14:57:44.429000000          86.290821   \n",
       "2  2016-01-09 05:36:02.389000000          85.512541   \n",
       "3  2016-04-30 06:50:39.801000000          68.824532   \n",
       "4  2015-12-13 17:06:10.750000000          51.491040   \n",
       "\n",
       "          assignment2_submission  assignment3_grade  \\\n",
       "0  2015-11-09 02:22:58.938000000          67.164441   \n",
       "1  2015-12-06 17:41:18.449000000          69.772657   \n",
       "2  2016-01-09 06:39:44.416000000          68.410033   \n",
       "3  2016-04-30 17:20:38.727000000          61.942079   \n",
       "4  2015-12-14 12:25:12.056000000          41.932832   \n",
       "\n",
       "          assignment3_submission  assignment4_grade  \\\n",
       "0  2015-11-12 08:58:33.998000000          53.011553   \n",
       "1  2015-12-10 08:54:55.904000000          55.098125   \n",
       "2  2016-01-15 20:22:45.882000000          54.728026   \n",
       "3  2016-05-12 07:47:16.326000000          49.553663   \n",
       "4  2015-12-29 14:25:22.594000000          36.929549   \n",
       "\n",
       "          assignment4_submission  assignment5_grade  \\\n",
       "0  2015-11-16 01:21:24.663000000          47.710398   \n",
       "1  2015-12-13 17:32:30.941000000          49.588313   \n",
       "2  2016-01-11 12:41:50.749000000          49.255224   \n",
       "3  2016-05-07 16:09:20.485000000          49.553663   \n",
       "4  2015-12-28 01:29:55.901000000          33.236594   \n",
       "\n",
       "          assignment5_submission  assignment6_grade  \\\n",
       "0  2015-11-20 13:24:59.692000000          38.168318   \n",
       "1  2015-12-19 23:26:39.285000000          44.629482   \n",
       "2  2016-01-11 17:31:12.489000000          44.329701   \n",
       "3  2016-05-24 12:51:18.016000000          44.598297   \n",
       "4  2015-12-29 14:46:06.628000000          33.236594   \n",
       "\n",
       "          assignment6_submission  \n",
       "0  2015-11-22 18:31:15.934000000  \n",
       "1  2015-12-21 17:07:24.275000000  \n",
       "2  2016-01-17 16:24:42.765000000  \n",
       "3  2016-05-26 08:09:12.058000000  \n",
       "4  2016-01-05 01:06:59.546000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cuando hacemos hipotesis testing, realmente tenemos dos statements de interes: \n",
    "# El primero es nuestra actual explanation, que podemos llamar alternative hypothesis, y el secundo es \n",
    "# que la explanation que tenemos no es suficiente y llamamos a esto the null hypothesis\n",
    "# Our actual testing method es determinar si nuestra null hypothesis es true or not. Si encontramos que hay\n",
    "# diferencias entre grupos, podemos rechazar the null hypothesis y aceptar nuestra alternativa\n",
    "\n",
    "\n",
    "# Veamos un ejemplo, usaremos alguna data de notas\n",
    "df = pd.read_csv('resources/week-4/datasets/grades.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2315 rows and 13 columns\n"
     ]
    }
   ],
   "source": [
    "# Si vemos nuestro dataframe, vemos que hay seis diferentes asignaciones. Veamos algunos \n",
    "# resumenes estadisticos para este DataFrame\n",
    "\n",
    "print('There are {} rows and {} columns'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>assignment1_grade</th>\n",
       "      <th>assignment1_submission</th>\n",
       "      <th>assignment2_grade</th>\n",
       "      <th>assignment2_submission</th>\n",
       "      <th>assignment3_grade</th>\n",
       "      <th>assignment3_submission</th>\n",
       "      <th>assignment4_grade</th>\n",
       "      <th>assignment4_submission</th>\n",
       "      <th>assignment5_grade</th>\n",
       "      <th>assignment5_submission</th>\n",
       "      <th>assignment6_grade</th>\n",
       "      <th>assignment6_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B73F2C11-70F0-E37D-8B10-1D20AFED50B1</td>\n",
       "      <td>92.733946</td>\n",
       "      <td>2015-11-02 06:55:34.282000000</td>\n",
       "      <td>83.030552</td>\n",
       "      <td>2015-11-09 02:22:58.938000000</td>\n",
       "      <td>67.164441</td>\n",
       "      <td>2015-11-12 08:58:33.998000000</td>\n",
       "      <td>53.011553</td>\n",
       "      <td>2015-11-16 01:21:24.663000000</td>\n",
       "      <td>47.710398</td>\n",
       "      <td>2015-11-20 13:24:59.692000000</td>\n",
       "      <td>38.168318</td>\n",
       "      <td>2015-11-22 18:31:15.934000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1</td>\n",
       "      <td>86.790821</td>\n",
       "      <td>2015-11-29 14:57:44.429000000</td>\n",
       "      <td>86.290821</td>\n",
       "      <td>2015-12-06 17:41:18.449000000</td>\n",
       "      <td>69.772657</td>\n",
       "      <td>2015-12-10 08:54:55.904000000</td>\n",
       "      <td>55.098125</td>\n",
       "      <td>2015-12-13 17:32:30.941000000</td>\n",
       "      <td>49.588313</td>\n",
       "      <td>2015-12-19 23:26:39.285000000</td>\n",
       "      <td>44.629482</td>\n",
       "      <td>2015-12-21 17:07:24.275000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5ECBEEB6-F1CE-80AE-3164-E45E99473FB4</td>\n",
       "      <td>64.813800</td>\n",
       "      <td>2015-12-13 17:06:10.750000000</td>\n",
       "      <td>51.491040</td>\n",
       "      <td>2015-12-14 12:25:12.056000000</td>\n",
       "      <td>41.932832</td>\n",
       "      <td>2015-12-29 14:25:22.594000000</td>\n",
       "      <td>36.929549</td>\n",
       "      <td>2015-12-28 01:29:55.901000000</td>\n",
       "      <td>33.236594</td>\n",
       "      <td>2015-12-29 14:46:06.628000000</td>\n",
       "      <td>33.236594</td>\n",
       "      <td>2016-01-05 01:06:59.546000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D09000A0-827B-C0FF-3433-BF8FF286E15B</td>\n",
       "      <td>71.647278</td>\n",
       "      <td>2015-12-28 04:35:32.836000000</td>\n",
       "      <td>64.052550</td>\n",
       "      <td>2016-01-03 21:05:38.392000000</td>\n",
       "      <td>64.752550</td>\n",
       "      <td>2016-01-07 08:55:43.692000000</td>\n",
       "      <td>57.467295</td>\n",
       "      <td>2016-01-11 00:45:28.706000000</td>\n",
       "      <td>57.467295</td>\n",
       "      <td>2016-01-11 00:54:13.579000000</td>\n",
       "      <td>57.467295</td>\n",
       "      <td>2016-01-20 19:54:46.166000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C9D51293-BD58-F113-4167-A7C0BAFCB6E5</td>\n",
       "      <td>66.595568</td>\n",
       "      <td>2015-12-25 02:29:28.415000000</td>\n",
       "      <td>52.916454</td>\n",
       "      <td>2015-12-31 01:42:30.046000000</td>\n",
       "      <td>48.344809</td>\n",
       "      <td>2016-01-05 23:34:02.180000000</td>\n",
       "      <td>47.444809</td>\n",
       "      <td>2016-01-02 07:48:42.517000000</td>\n",
       "      <td>37.955847</td>\n",
       "      <td>2016-01-03 21:27:04.266000000</td>\n",
       "      <td>37.955847</td>\n",
       "      <td>2016-01-19 15:24:31.060000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             student_id  assignment1_grade  \\\n",
       "0  B73F2C11-70F0-E37D-8B10-1D20AFED50B1          92.733946   \n",
       "1  98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1          86.790821   \n",
       "4  5ECBEEB6-F1CE-80AE-3164-E45E99473FB4          64.813800   \n",
       "5  D09000A0-827B-C0FF-3433-BF8FF286E15B          71.647278   \n",
       "8  C9D51293-BD58-F113-4167-A7C0BAFCB6E5          66.595568   \n",
       "\n",
       "          assignment1_submission  assignment2_grade  \\\n",
       "0  2015-11-02 06:55:34.282000000          83.030552   \n",
       "1  2015-11-29 14:57:44.429000000          86.290821   \n",
       "4  2015-12-13 17:06:10.750000000          51.491040   \n",
       "5  2015-12-28 04:35:32.836000000          64.052550   \n",
       "8  2015-12-25 02:29:28.415000000          52.916454   \n",
       "\n",
       "          assignment2_submission  assignment3_grade  \\\n",
       "0  2015-11-09 02:22:58.938000000          67.164441   \n",
       "1  2015-12-06 17:41:18.449000000          69.772657   \n",
       "4  2015-12-14 12:25:12.056000000          41.932832   \n",
       "5  2016-01-03 21:05:38.392000000          64.752550   \n",
       "8  2015-12-31 01:42:30.046000000          48.344809   \n",
       "\n",
       "          assignment3_submission  assignment4_grade  \\\n",
       "0  2015-11-12 08:58:33.998000000          53.011553   \n",
       "1  2015-12-10 08:54:55.904000000          55.098125   \n",
       "4  2015-12-29 14:25:22.594000000          36.929549   \n",
       "5  2016-01-07 08:55:43.692000000          57.467295   \n",
       "8  2016-01-05 23:34:02.180000000          47.444809   \n",
       "\n",
       "          assignment4_submission  assignment5_grade  \\\n",
       "0  2015-11-16 01:21:24.663000000          47.710398   \n",
       "1  2015-12-13 17:32:30.941000000          49.588313   \n",
       "4  2015-12-28 01:29:55.901000000          33.236594   \n",
       "5  2016-01-11 00:45:28.706000000          57.467295   \n",
       "8  2016-01-02 07:48:42.517000000          37.955847   \n",
       "\n",
       "          assignment5_submission  assignment6_grade  \\\n",
       "0  2015-11-20 13:24:59.692000000          38.168318   \n",
       "1  2015-12-19 23:26:39.285000000          44.629482   \n",
       "4  2015-12-29 14:46:06.628000000          33.236594   \n",
       "5  2016-01-11 00:54:13.579000000          57.467295   \n",
       "8  2016-01-03 21:27:04.266000000          37.955847   \n",
       "\n",
       "          assignment6_submission  \n",
       "0  2015-11-22 18:31:15.934000000  \n",
       "1  2015-12-21 17:07:24.275000000  \n",
       "4  2016-01-05 01:06:59.546000000  \n",
       "5  2016-01-20 19:54:46.166000000  \n",
       "8  2016-01-19 15:24:31.060000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a segmentar esta poblacion en dos piezas. Diremos que quienes terminaron la primera asignacion\n",
    "# al final de December 2015 seran early finishers, y aquellos que la terminaron un tiempo despues seran late finishers\n",
    "\n",
    "early_finishers = df[pd.to_datetime(df['assignment1_submission']) < '2016'] # Pasamos la columna del df a datetime\n",
    "early_finishers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>assignment1_grade</th>\n",
       "      <th>assignment1_submission</th>\n",
       "      <th>assignment2_grade</th>\n",
       "      <th>assignment2_submission</th>\n",
       "      <th>assignment3_grade</th>\n",
       "      <th>assignment3_submission</th>\n",
       "      <th>assignment4_grade</th>\n",
       "      <th>assignment4_submission</th>\n",
       "      <th>assignment5_grade</th>\n",
       "      <th>assignment5_submission</th>\n",
       "      <th>assignment6_grade</th>\n",
       "      <th>assignment6_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0F62040-CEB0-904C-F563-2F8620916C4E</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 05:36:02.389000000</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 06:39:44.416000000</td>\n",
       "      <td>68.410033</td>\n",
       "      <td>2016-01-15 20:22:45.882000000</td>\n",
       "      <td>54.728026</td>\n",
       "      <td>2016-01-11 12:41:50.749000000</td>\n",
       "      <td>49.255224</td>\n",
       "      <td>2016-01-11 17:31:12.489000000</td>\n",
       "      <td>44.329701</td>\n",
       "      <td>2016-01-17 16:24:42.765000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FFDF2B2C-F514-EF7F-6538-A6A53518E9DC</td>\n",
       "      <td>86.030665</td>\n",
       "      <td>2016-04-30 06:50:39.801000000</td>\n",
       "      <td>68.824532</td>\n",
       "      <td>2016-04-30 17:20:38.727000000</td>\n",
       "      <td>61.942079</td>\n",
       "      <td>2016-05-12 07:47:16.326000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-07 16:09:20.485000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-24 12:51:18.016000000</td>\n",
       "      <td>44.598297</td>\n",
       "      <td>2016-05-26 08:09:12.058000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3217BE3F-E4B0-C3B6-9F64-462456819CE4</td>\n",
       "      <td>87.498744</td>\n",
       "      <td>2016-03-05 11:05:25.408000000</td>\n",
       "      <td>69.998995</td>\n",
       "      <td>2016-03-09 07:29:52.405000000</td>\n",
       "      <td>55.999196</td>\n",
       "      <td>2016-03-16 22:31:24.316000000</td>\n",
       "      <td>50.399276</td>\n",
       "      <td>2016-03-18 07:19:26.032000000</td>\n",
       "      <td>45.359349</td>\n",
       "      <td>2016-03-19 10:35:41.869000000</td>\n",
       "      <td>45.359349</td>\n",
       "      <td>2016-03-23 14:02:00.987000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1CB5AA1-B3DE-5460-FAFF-BE951FD38B5F</td>\n",
       "      <td>80.576090</td>\n",
       "      <td>2016-01-24 18:24:25.619000000</td>\n",
       "      <td>72.518481</td>\n",
       "      <td>2016-01-27 13:37:12.943000000</td>\n",
       "      <td>65.266633</td>\n",
       "      <td>2016-01-30 14:34:36.581000000</td>\n",
       "      <td>65.266633</td>\n",
       "      <td>2016-02-03 22:08:49.002000000</td>\n",
       "      <td>65.266633</td>\n",
       "      <td>2016-02-16 14:22:23.664000000</td>\n",
       "      <td>65.266633</td>\n",
       "      <td>2016-02-18 08:35:04.796000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>E2C617C2-4654-622C-AB50-1550C4BE42A0</td>\n",
       "      <td>59.270882</td>\n",
       "      <td>2016-03-06 12:06:26.185000000</td>\n",
       "      <td>59.270882</td>\n",
       "      <td>2016-03-13 02:07:25.289000000</td>\n",
       "      <td>53.343794</td>\n",
       "      <td>2016-03-17 07:30:09.241000000</td>\n",
       "      <td>53.343794</td>\n",
       "      <td>2016-03-20 21:45:56.229000000</td>\n",
       "      <td>42.675035</td>\n",
       "      <td>2016-03-27 15:55:04.414000000</td>\n",
       "      <td>38.407532</td>\n",
       "      <td>2016-03-30 20:33:13.554000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             student_id  assignment1_grade  \\\n",
       "2  D0F62040-CEB0-904C-F563-2F8620916C4E          85.512541   \n",
       "3  FFDF2B2C-F514-EF7F-6538-A6A53518E9DC          86.030665   \n",
       "6  3217BE3F-E4B0-C3B6-9F64-462456819CE4          87.498744   \n",
       "7  F1CB5AA1-B3DE-5460-FAFF-BE951FD38B5F          80.576090   \n",
       "9  E2C617C2-4654-622C-AB50-1550C4BE42A0          59.270882   \n",
       "\n",
       "          assignment1_submission  assignment2_grade  \\\n",
       "2  2016-01-09 05:36:02.389000000          85.512541   \n",
       "3  2016-04-30 06:50:39.801000000          68.824532   \n",
       "6  2016-03-05 11:05:25.408000000          69.998995   \n",
       "7  2016-01-24 18:24:25.619000000          72.518481   \n",
       "9  2016-03-06 12:06:26.185000000          59.270882   \n",
       "\n",
       "          assignment2_submission  assignment3_grade  \\\n",
       "2  2016-01-09 06:39:44.416000000          68.410033   \n",
       "3  2016-04-30 17:20:38.727000000          61.942079   \n",
       "6  2016-03-09 07:29:52.405000000          55.999196   \n",
       "7  2016-01-27 13:37:12.943000000          65.266633   \n",
       "9  2016-03-13 02:07:25.289000000          53.343794   \n",
       "\n",
       "          assignment3_submission  assignment4_grade  \\\n",
       "2  2016-01-15 20:22:45.882000000          54.728026   \n",
       "3  2016-05-12 07:47:16.326000000          49.553663   \n",
       "6  2016-03-16 22:31:24.316000000          50.399276   \n",
       "7  2016-01-30 14:34:36.581000000          65.266633   \n",
       "9  2016-03-17 07:30:09.241000000          53.343794   \n",
       "\n",
       "          assignment4_submission  assignment5_grade  \\\n",
       "2  2016-01-11 12:41:50.749000000          49.255224   \n",
       "3  2016-05-07 16:09:20.485000000          49.553663   \n",
       "6  2016-03-18 07:19:26.032000000          45.359349   \n",
       "7  2016-02-03 22:08:49.002000000          65.266633   \n",
       "9  2016-03-20 21:45:56.229000000          42.675035   \n",
       "\n",
       "          assignment5_submission  assignment6_grade  \\\n",
       "2  2016-01-11 17:31:12.489000000          44.329701   \n",
       "3  2016-05-24 12:51:18.016000000          44.598297   \n",
       "6  2016-03-19 10:35:41.869000000          45.359349   \n",
       "7  2016-02-16 14:22:23.664000000          65.266633   \n",
       "9  2016-03-27 15:55:04.414000000          38.407532   \n",
       "\n",
       "          assignment6_submission  \n",
       "2  2016-01-17 16:24:42.765000000  \n",
       "3  2016-05-26 08:09:12.058000000  \n",
       "6  2016-03-23 14:02:00.987000000  \n",
       "7  2016-02-18 08:35:04.796000000  \n",
       "9  2016-03-30 20:33:13.554000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hay varias formas de tomar los late finishers. Por ejemplo, podriamos consultar de la misma forma pero pidiendo\n",
    "# que sea > 2016. En este caso, sabiendo que early finishers y df tienen el mismo index, realmente estamos buscando\n",
    "# aquellos estudiantes que no esten en early finishers\n",
    "\n",
    "late_finishers= df[~df.index.isin(early_finishers.index)]\n",
    "late_finishers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otra forma seria hacer una interseccion del df con early_finishers, si hacemos un left join solo nos quedaran\n",
    "# aquellos items en el left dataframe. Tambien se puede escribir una funcion que determine si alguien es\n",
    "# early or late y luego llamar .apply() y añadir una nueva columna al dataframe. Hay un numero razonable de cosas\n",
    "# que pueden hacerse para esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El Pandas DataFrame object tiene una variedad de funciones estadisticas asociadas a el. Podemos llamar\n",
    "# estas funciones directamente en el dataframe. Veamos los promedios para nuestras dos poblaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.94728457024304\n",
      "74.0450648477065\n"
     ]
    }
   ],
   "source": [
    "print(early_finishers['assignment1_grade'].mean())\n",
    "print(late_finishers['assignment1_grade'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.3223540853721596, pvalue=0.18618101101713855)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se ven muy similares. Pero son los mismos? Que queremos decir con similar? Aqui es donde\n",
    "# los t-test entran. Esto permite formar una alternative hypothesis('There are different' ) y tambien una\n",
    "# null hypothesis ('There are the same') e intentar probar esta null hypothesis.\n",
    "\n",
    "# Cuando hacemos hypothesis testing, tenemos que elegir un nivel de significancia (significance level) como\n",
    "# de cuanto chance le vamos a dar para aceptarlas. El significance level suele llamarse alpha. Por ejemplo,\n",
    "# podriamos establecer un alpha de 0.05 o de 5%. Este es un valor comun para utilizar pero es un poco\n",
    "# arbitrario\n",
    "\n",
    "\n",
    "# L libreria de SciPy contiene un numero de diferentes statistical tests y forms para basar una hypothesis \n",
    "# testing en python. Usaremos ttest_ind() que hace independientes t-test (meaning the population are not related to one another)\n",
    "# El resultado del ttest_index() son las t-statistic y un p-value. Este ultimo valor es la probabilidad,\n",
    "# que es muy importante para nosotros y nos indica el chance (entre 0 y 1) o la posibilidad de que nuestra \n",
    "# null hypothesis sea verdadera\n",
    "\n",
    "# vamos a importar ttest_ind function\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Ahora corramos esta funcion con nuestras dos poblaciones\n",
    "ttest_ind(early_finishers['assignment1_grade'], late_finishers['assignment1_grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui vemos que la probabildiad es 0.18, esto esta por encima de nuestro alpha de 0.05. Esto quiere decir que \n",
    "# no podemos rechazar nuestra null hypothesis. La null hypothesis fue que ambas poblaciones eran las mismas y no tenemos\n",
    "# suficiente certeza en nuestra evidencia (porque es mayor que alpha) para sacar una conclusion de lo contrario\n",
    "# Esto tampoco quiere decir que las poblaciones sean las mismas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=1.2514717608216366, pvalue=0.21088896270044244)\n",
      "Ttest_indResult(statistic=1.6133726558705392, pvalue=0.1067999810222786)\n",
      "Ttest_indResult(statistic=0.049671157386456125, pvalue=0.960388729789337)\n",
      "Ttest_indResult(statistic=-0.05279315545404755, pvalue=0.9579012739746492)\n",
      "Ttest_indResult(statistic=-0.11609743352609489, pvalue=0.9075854011989859)\n"
     ]
    }
   ],
   "source": [
    "# Por que no miramos los otros assignment grades?\n",
    "\n",
    "print(ttest_ind(early_finishers['assignment2_grade'], late_finishers['assignment2_grade']))\n",
    "print(ttest_ind(early_finishers['assignment3_grade'], late_finishers['assignment3_grade']))\n",
    "print(ttest_ind(early_finishers['assignment4_grade'], late_finishers['assignment4_grade']))\n",
    "print(ttest_ind(early_finishers['assignment5_grade'], late_finishers['assignment5_grade']))\n",
    "print(ttest_ind(early_finishers['assignment6_grade'], late_finishers['assignment6_grade']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bueno, por lo que vemos en esta data no tenemos suficiente evidencia que sugiera que las poblaciones difieren\n",
    "# respecto al grade. Veamos estos pvalue por un momento, Por ejemplo, uno de las asignaciones, el assignment 3\n",
    "# tiene un p-value around 0.1. Esto significa que si aceptamos un nivel de chance similarity de 11% esto seria\n",
    "# considerado estadisticamente significativo. Como una investigacion, esto podria sugerir que hay algo aqui que\n",
    "# vale la pena cnsiderar seguir. Por ejemplo, si tuvieramos un pequeño numero de participantes (que no es asi)\n",
    "# o si habia algo unico en esta tarea ya que se relaciona con el experimento (sea cual sea) puede haber diferentes\n",
    "# experimentos que podriamos correr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631622</td>\n",
       "      <td>0.980429</td>\n",
       "      <td>0.541378</td>\n",
       "      <td>0.675257</td>\n",
       "      <td>0.594209</td>\n",
       "      <td>0.126157</td>\n",
       "      <td>0.677277</td>\n",
       "      <td>0.733248</td>\n",
       "      <td>0.542124</td>\n",
       "      <td>0.375118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119582</td>\n",
       "      <td>0.211202</td>\n",
       "      <td>0.842628</td>\n",
       "      <td>0.698714</td>\n",
       "      <td>0.588570</td>\n",
       "      <td>0.766740</td>\n",
       "      <td>0.519179</td>\n",
       "      <td>0.304661</td>\n",
       "      <td>0.327297</td>\n",
       "      <td>0.356673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.141092</td>\n",
       "      <td>0.293658</td>\n",
       "      <td>0.956149</td>\n",
       "      <td>0.982746</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.346991</td>\n",
       "      <td>0.652229</td>\n",
       "      <td>0.282293</td>\n",
       "      <td>0.068041</td>\n",
       "      <td>0.089381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736282</td>\n",
       "      <td>0.449924</td>\n",
       "      <td>0.179042</td>\n",
       "      <td>0.946815</td>\n",
       "      <td>0.181816</td>\n",
       "      <td>0.783499</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.245215</td>\n",
       "      <td>0.603233</td>\n",
       "      <td>0.255904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.383116</td>\n",
       "      <td>0.898514</td>\n",
       "      <td>0.432710</td>\n",
       "      <td>0.422635</td>\n",
       "      <td>0.889698</td>\n",
       "      <td>0.136309</td>\n",
       "      <td>0.717120</td>\n",
       "      <td>0.834116</td>\n",
       "      <td>0.869712</td>\n",
       "      <td>0.559002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263236</td>\n",
       "      <td>0.759099</td>\n",
       "      <td>0.943630</td>\n",
       "      <td>0.833461</td>\n",
       "      <td>0.475799</td>\n",
       "      <td>0.610442</td>\n",
       "      <td>0.799343</td>\n",
       "      <td>0.989965</td>\n",
       "      <td>0.496995</td>\n",
       "      <td>0.232954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.857676</td>\n",
       "      <td>0.790472</td>\n",
       "      <td>0.307499</td>\n",
       "      <td>0.653475</td>\n",
       "      <td>0.012058</td>\n",
       "      <td>0.662111</td>\n",
       "      <td>0.909895</td>\n",
       "      <td>0.398625</td>\n",
       "      <td>0.787891</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488357</td>\n",
       "      <td>0.099418</td>\n",
       "      <td>0.258302</td>\n",
       "      <td>0.522440</td>\n",
       "      <td>0.033740</td>\n",
       "      <td>0.390086</td>\n",
       "      <td>0.180167</td>\n",
       "      <td>0.473175</td>\n",
       "      <td>0.085052</td>\n",
       "      <td>0.034355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.497272</td>\n",
       "      <td>0.816769</td>\n",
       "      <td>0.888165</td>\n",
       "      <td>0.779664</td>\n",
       "      <td>0.883219</td>\n",
       "      <td>0.225290</td>\n",
       "      <td>0.250772</td>\n",
       "      <td>0.447332</td>\n",
       "      <td>0.015132</td>\n",
       "      <td>0.412379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581744</td>\n",
       "      <td>0.488824</td>\n",
       "      <td>0.380911</td>\n",
       "      <td>0.234609</td>\n",
       "      <td>0.853153</td>\n",
       "      <td>0.423854</td>\n",
       "      <td>0.489200</td>\n",
       "      <td>0.573071</td>\n",
       "      <td>0.287947</td>\n",
       "      <td>0.154285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.631622  0.980429  0.541378  0.675257  0.594209  0.126157  0.677277   \n",
       "1  0.141092  0.293658  0.956149  0.982746  0.281000  0.346991  0.652229   \n",
       "2  0.383116  0.898514  0.432710  0.422635  0.889698  0.136309  0.717120   \n",
       "3  0.857676  0.790472  0.307499  0.653475  0.012058  0.662111  0.909895   \n",
       "4  0.497272  0.816769  0.888165  0.779664  0.883219  0.225290  0.250772   \n",
       "\n",
       "         7         8         9     ...           90        91        92  \\\n",
       "0  0.733248  0.542124  0.375118    ...     0.119582  0.211202  0.842628   \n",
       "1  0.282293  0.068041  0.089381    ...     0.736282  0.449924  0.179042   \n",
       "2  0.834116  0.869712  0.559002    ...     0.263236  0.759099  0.943630   \n",
       "3  0.398625  0.787891  0.384000    ...     0.488357  0.099418  0.258302   \n",
       "4  0.447332  0.015132  0.412379    ...     0.581744  0.488824  0.380911   \n",
       "\n",
       "         93        94        95        96        97        98        99  \n",
       "0  0.698714  0.588570  0.766740  0.519179  0.304661  0.327297  0.356673  \n",
       "1  0.946815  0.181816  0.783499  0.021885  0.245215  0.603233  0.255904  \n",
       "2  0.833461  0.475799  0.610442  0.799343  0.989965  0.496995  0.232954  \n",
       "3  0.522440  0.033740  0.390086  0.180167  0.473175  0.085052  0.034355  \n",
       "4  0.234609  0.853153  0.423854  0.489200  0.573071  0.287947  0.154285  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P-value han sido atacados recientemente por ser insuficientes para decirnos sobre las interacciones que esta\n",
    "# sucediento y otras tecnicas, como intervalos de confianza y analisis bayesianos, se estan utilizando con mas \n",
    "# regularidad\n",
    "\n",
    "# Un asunto con p-values es que podemos correr mas tests, a medidas que ejecutas mas pruebas, es probable que obtengas\n",
    "# un valor estadisticamente significativo solo por casualidad. Veamos una pequeña simulacion de esto.\n",
    "\n",
    "\n",
    "# Vamos a crear un df de 100 columnas cada una con 100 numeros\n",
    "df1 = pd.DataFrame([np.random.random(100) for x in range(100)])\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.798359</td>\n",
       "      <td>0.824411</td>\n",
       "      <td>0.163510</td>\n",
       "      <td>0.679847</td>\n",
       "      <td>0.130451</td>\n",
       "      <td>0.961485</td>\n",
       "      <td>0.897837</td>\n",
       "      <td>0.226593</td>\n",
       "      <td>0.106009</td>\n",
       "      <td>0.285651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706896</td>\n",
       "      <td>0.458154</td>\n",
       "      <td>0.868299</td>\n",
       "      <td>0.537835</td>\n",
       "      <td>0.529920</td>\n",
       "      <td>0.090139</td>\n",
       "      <td>0.442255</td>\n",
       "      <td>0.084874</td>\n",
       "      <td>0.397607</td>\n",
       "      <td>0.027577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.905959</td>\n",
       "      <td>0.806044</td>\n",
       "      <td>0.795189</td>\n",
       "      <td>0.689599</td>\n",
       "      <td>0.589827</td>\n",
       "      <td>0.576660</td>\n",
       "      <td>0.078990</td>\n",
       "      <td>0.838910</td>\n",
       "      <td>0.851161</td>\n",
       "      <td>0.839004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806992</td>\n",
       "      <td>0.322622</td>\n",
       "      <td>0.546312</td>\n",
       "      <td>0.543335</td>\n",
       "      <td>0.294068</td>\n",
       "      <td>0.094001</td>\n",
       "      <td>0.956558</td>\n",
       "      <td>0.972448</td>\n",
       "      <td>0.896301</td>\n",
       "      <td>0.570958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.900753</td>\n",
       "      <td>0.968876</td>\n",
       "      <td>0.875826</td>\n",
       "      <td>0.681239</td>\n",
       "      <td>0.209147</td>\n",
       "      <td>0.032033</td>\n",
       "      <td>0.279006</td>\n",
       "      <td>0.051622</td>\n",
       "      <td>0.940853</td>\n",
       "      <td>0.818897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718430</td>\n",
       "      <td>0.274410</td>\n",
       "      <td>0.122726</td>\n",
       "      <td>0.250765</td>\n",
       "      <td>0.528643</td>\n",
       "      <td>0.237594</td>\n",
       "      <td>0.419881</td>\n",
       "      <td>0.158310</td>\n",
       "      <td>0.060842</td>\n",
       "      <td>0.255180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024976</td>\n",
       "      <td>0.277030</td>\n",
       "      <td>0.617675</td>\n",
       "      <td>0.460331</td>\n",
       "      <td>0.892588</td>\n",
       "      <td>0.903590</td>\n",
       "      <td>0.417025</td>\n",
       "      <td>0.610786</td>\n",
       "      <td>0.434334</td>\n",
       "      <td>0.289827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200820</td>\n",
       "      <td>0.250120</td>\n",
       "      <td>0.156291</td>\n",
       "      <td>0.530534</td>\n",
       "      <td>0.653799</td>\n",
       "      <td>0.893017</td>\n",
       "      <td>0.480380</td>\n",
       "      <td>0.070557</td>\n",
       "      <td>0.373636</td>\n",
       "      <td>0.323213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.136186</td>\n",
       "      <td>0.314427</td>\n",
       "      <td>0.670367</td>\n",
       "      <td>0.321676</td>\n",
       "      <td>0.069062</td>\n",
       "      <td>0.874041</td>\n",
       "      <td>0.217379</td>\n",
       "      <td>0.650205</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.863905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689687</td>\n",
       "      <td>0.589834</td>\n",
       "      <td>0.045550</td>\n",
       "      <td>0.215566</td>\n",
       "      <td>0.660558</td>\n",
       "      <td>0.370395</td>\n",
       "      <td>0.398110</td>\n",
       "      <td>0.633917</td>\n",
       "      <td>0.042203</td>\n",
       "      <td>0.251415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.798359  0.824411  0.163510  0.679847  0.130451  0.961485  0.897837   \n",
       "1  0.905959  0.806044  0.795189  0.689599  0.589827  0.576660  0.078990   \n",
       "2  0.900753  0.968876  0.875826  0.681239  0.209147  0.032033  0.279006   \n",
       "3  0.024976  0.277030  0.617675  0.460331  0.892588  0.903590  0.417025   \n",
       "4  0.136186  0.314427  0.670367  0.321676  0.069062  0.874041  0.217379   \n",
       "\n",
       "         7         8         9     ...           90        91        92  \\\n",
       "0  0.226593  0.106009  0.285651    ...     0.706896  0.458154  0.868299   \n",
       "1  0.838910  0.851161  0.839004    ...     0.806992  0.322622  0.546312   \n",
       "2  0.051622  0.940853  0.818897    ...     0.718430  0.274410  0.122726   \n",
       "3  0.610786  0.434334  0.289827    ...     0.200820  0.250120  0.156291   \n",
       "4  0.650205  0.001485  0.863905    ...     0.689687  0.589834  0.045550   \n",
       "\n",
       "         93        94        95        96        97        98        99  \n",
       "0  0.537835  0.529920  0.090139  0.442255  0.084874  0.397607  0.027577  \n",
       "1  0.543335  0.294068  0.094001  0.956558  0.972448  0.896301  0.570958  \n",
       "2  0.250765  0.528643  0.237594  0.419881  0.158310  0.060842  0.255180  \n",
       "3  0.530534  0.653799  0.893017  0.480380  0.070557  0.373636  0.323213  \n",
       "4  0.215566  0.660558  0.370395  0.398110  0.633917  0.042203  0.251415  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creemos un segundo dataframe\n",
    "\n",
    "df2 = pd.DataFrame([np.random.random(100) for x in range(100)])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col 19 is statistically significantly different at alpha=0.1, pval=0.05084279351359974\n",
      "Col 41 is statistically significantly different at alpha=0.1, pval=0.06859568063257189\n",
      "Col 47 is statistically significantly different at alpha=0.1, pval=0.05089053080235727\n",
      "Col 60 is statistically significantly different at alpha=0.1, pval=0.055192627504264284\n",
      "Col 69 is statistically significantly different at alpha=0.1, pval=0.0911505695191257\n",
      "Col 70 is statistically significantly different at alpha=0.1, pval=0.006981454838530834\n",
      "Col 76 is statistically significantly different at alpha=0.1, pval=0.05463199062992337\n",
      "Col 78 is statistically significantly different at alpha=0.1, pval=0.0735626502049387\n",
      "Col 83 is statistically significantly different at alpha=0.1, pval=0.07212426894988395\n",
      "Col 87 is statistically significantly different at alpha=0.1, pval=0.02219707331086324\n",
      "Col 88 is statistically significantly different at alpha=0.1, pval=0.09415341863524902\n",
      "Col 95 is statistically significantly different at alpha=0.1, pval=0.0010441104480407234\n",
      "Total number diferrent was 12, which is 12.0%\n"
     ]
    }
   ],
   "source": [
    "# Entonces, son estos df iguales? Por una fila dentro de df1, es esta la misma que la fila dentro de df2\n",
    "\n",
    "# Veamos esto, digamos que neustro critical value es 0.1 o un alpha de 10%,  vamos a comparar \n",
    "# cada columna en df1 con la misma columna en df2 (same numered) y vamos a reportar el p-value\n",
    "# a ver si es menor del 10%, que significaria que tenemos suficiente evidencia para decir que las columnas \n",
    "# diferentes.\n",
    "\n",
    "# Vamos a escribir esto en una funcion llamada test_columns\n",
    "\n",
    "def test_columns(alpha=0.1):\n",
    "    # Queremos tener track de cuantos difieren\n",
    "    num_diff= 0\n",
    "    # Ahora queremos iterar sobre las columnas\n",
    "    for col in df1.columns:\n",
    "        # Podemos correr ttest_ind entre dos dataframes\n",
    "        teststat,pval=ttest_ind(df1[col],df2[col])\n",
    "        \n",
    "        # Podemos revisar el pvalue versus the alpha\n",
    "        if pval <= alpha:\n",
    "            # Podemos imprimir si son diferentes e incrementar num_diff\n",
    "            print('Col {} is statistically significantly different at alpha={}, pval={}'.format(col,alpha,pval))\n",
    "            num_diff=num_diff+1\n",
    "            \n",
    "        # Y tambien podemos imprimir algun summary stats\n",
    "    print('Total number diferrent was {}, which is {}%'.format(num_diff,float(num_diff)/len(df1.columns)*100))\n",
    "# Ahora corramos el codigo\n",
    "test_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col 70 is statistically significantly different at alpha=0.05, pval=0.006981454838530834\n",
      "Col 87 is statistically significantly different at alpha=0.05, pval=0.02219707331086324\n",
      "Col 95 is statistically significantly different at alpha=0.05, pval=0.0010441104480407234\n",
      "Total number diferrent was 3, which is 3.0%\n"
     ]
    }
   ],
   "source": [
    "# Interesante. Vemos que tenemos un puñado de columnas que son diferentes. De hecho, ese numero se ve\n",
    "# muy parecido al valor de alpha que elegimos. Que ocurre? No deberian ser todas las columnas las mismas?\n",
    "# Recordemos que ttest hace un check si dos sets son similares segun un cierto nivel de confidencia dado, en nuestro\n",
    "# caso, 10%. Mientras mas comparaciones random hagas, mas pasara que sean la misma by chance. En este ejemplo\n",
    "# revisamos 100 columns por lo que esperariamos que there to be roghly 10 of them if your alpha was 0.1\n",
    "\n",
    "# Podemos probar otro alpha value tambien\n",
    "test_columns(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col 0 is statistically significantly different at alpha=0.1, pval=0.0002227949102324349\n",
      "Col 1 is statistically significantly different at alpha=0.1, pval=0.0008907084452604087\n",
      "Col 2 is statistically significantly different at alpha=0.1, pval=9.175068898692099e-05\n",
      "Col 3 is statistically significantly different at alpha=0.1, pval=0.00029501341217386535\n",
      "Col 4 is statistically significantly different at alpha=0.1, pval=0.0009243885762726112\n",
      "Col 5 is statistically significantly different at alpha=0.1, pval=0.0013708906467274967\n",
      "Col 6 is statistically significantly different at alpha=0.1, pval=0.0003550034535231731\n",
      "Col 7 is statistically significantly different at alpha=0.1, pval=2.6142125306450425e-05\n",
      "Col 8 is statistically significantly different at alpha=0.1, pval=0.0003864194590648553\n",
      "Col 9 is statistically significantly different at alpha=0.1, pval=0.0003303901536382445\n",
      "Col 10 is statistically significantly different at alpha=0.1, pval=0.0006713039636976327\n",
      "Col 11 is statistically significantly different at alpha=0.1, pval=0.00218214677748732\n",
      "Col 12 is statistically significantly different at alpha=0.1, pval=4.416392644632646e-06\n",
      "Col 13 is statistically significantly different at alpha=0.1, pval=2.1179967880468946e-05\n",
      "Col 14 is statistically significantly different at alpha=0.1, pval=3.6990556783083834e-05\n",
      "Col 15 is statistically significantly different at alpha=0.1, pval=0.0002888515524584384\n",
      "Col 16 is statistically significantly different at alpha=0.1, pval=0.00022705917266848741\n",
      "Col 17 is statistically significantly different at alpha=0.1, pval=0.002142299283496547\n",
      "Col 18 is statistically significantly different at alpha=0.1, pval=0.00018918007739024127\n",
      "Col 19 is statistically significantly different at alpha=0.1, pval=0.024973643978455112\n",
      "Col 20 is statistically significantly different at alpha=0.1, pval=0.040167880409538205\n",
      "Col 21 is statistically significantly different at alpha=0.1, pval=1.3865404242828368e-05\n",
      "Col 22 is statistically significantly different at alpha=0.1, pval=0.00023908501466171452\n",
      "Col 23 is statistically significantly different at alpha=0.1, pval=7.916740679136545e-05\n",
      "Col 24 is statistically significantly different at alpha=0.1, pval=0.00022414101868275822\n",
      "Col 25 is statistically significantly different at alpha=0.1, pval=0.0064739983030066795\n",
      "Col 26 is statistically significantly different at alpha=0.1, pval=0.000382106811979183\n",
      "Col 27 is statistically significantly different at alpha=0.1, pval=0.0006985754710347831\n",
      "Col 28 is statistically significantly different at alpha=0.1, pval=0.00046901287637894835\n",
      "Col 29 is statistically significantly different at alpha=0.1, pval=0.004742132354753013\n",
      "Col 30 is statistically significantly different at alpha=0.1, pval=0.0003172862175020297\n",
      "Col 31 is statistically significantly different at alpha=0.1, pval=2.498443074268669e-06\n",
      "Col 32 is statistically significantly different at alpha=0.1, pval=0.004386043243654056\n",
      "Col 33 is statistically significantly different at alpha=0.1, pval=8.935362241740799e-05\n",
      "Col 34 is statistically significantly different at alpha=0.1, pval=0.0002980768874498339\n",
      "Col 35 is statistically significantly different at alpha=0.1, pval=0.004615531748747077\n",
      "Col 36 is statistically significantly different at alpha=0.1, pval=0.00045091598417435635\n",
      "Col 37 is statistically significantly different at alpha=0.1, pval=0.0016885864189733614\n",
      "Col 38 is statistically significantly different at alpha=0.1, pval=0.002186808147298542\n",
      "Col 39 is statistically significantly different at alpha=0.1, pval=3.216964838463318e-05\n",
      "Col 40 is statistically significantly different at alpha=0.1, pval=0.00019088055356663924\n",
      "Col 41 is statistically significantly different at alpha=0.1, pval=1.0286577006532524e-05\n",
      "Col 42 is statistically significantly different at alpha=0.1, pval=0.016495973155563105\n",
      "Col 43 is statistically significantly different at alpha=0.1, pval=0.001907460382355855\n",
      "Col 44 is statistically significantly different at alpha=0.1, pval=0.0008919510113088193\n",
      "Col 45 is statistically significantly different at alpha=0.1, pval=0.00414060713462633\n",
      "Col 46 is statistically significantly different at alpha=0.1, pval=0.003323874882536096\n",
      "Col 47 is statistically significantly different at alpha=0.1, pval=3.238678840881618e-05\n",
      "Col 48 is statistically significantly different at alpha=0.1, pval=0.01955291226145813\n",
      "Col 49 is statistically significantly different at alpha=0.1, pval=0.0007092362148419848\n",
      "Col 50 is statistically significantly different at alpha=0.1, pval=0.0010146034253996968\n",
      "Col 51 is statistically significantly different at alpha=0.1, pval=0.0012431388103435511\n",
      "Col 52 is statistically significantly different at alpha=0.1, pval=4.1197029269195126e-05\n",
      "Col 53 is statistically significantly different at alpha=0.1, pval=0.00024667483116228993\n",
      "Col 54 is statistically significantly different at alpha=0.1, pval=0.008570444885942604\n",
      "Col 55 is statistically significantly different at alpha=0.1, pval=0.00022678465638745494\n",
      "Col 56 is statistically significantly different at alpha=0.1, pval=0.002636601787640855\n",
      "Col 57 is statistically significantly different at alpha=0.1, pval=0.00030609604474892376\n",
      "Col 58 is statistically significantly different at alpha=0.1, pval=0.0003612624137763978\n",
      "Col 59 is statistically significantly different at alpha=0.1, pval=0.012679357919556204\n",
      "Col 60 is statistically significantly different at alpha=0.1, pval=7.998133745629313e-05\n",
      "Col 61 is statistically significantly different at alpha=0.1, pval=0.0037849297102377725\n",
      "Col 62 is statistically significantly different at alpha=0.1, pval=5.083883989113334e-05\n",
      "Col 63 is statistically significantly different at alpha=0.1, pval=0.00027054301266929757\n",
      "Col 64 is statistically significantly different at alpha=0.1, pval=0.00010351705644250619\n",
      "Col 65 is statistically significantly different at alpha=0.1, pval=0.00014809735384976386\n",
      "Col 66 is statistically significantly different at alpha=0.1, pval=0.002363249803945295\n",
      "Col 67 is statistically significantly different at alpha=0.1, pval=0.0004336516159691229\n",
      "Col 68 is statistically significantly different at alpha=0.1, pval=1.2581762620414703e-05\n",
      "Col 69 is statistically significantly different at alpha=0.1, pval=0.005657447510127677\n",
      "Col 70 is statistically significantly different at alpha=0.1, pval=0.00025855422072876413\n",
      "Col 71 is statistically significantly different at alpha=0.1, pval=0.0005259739130741366\n",
      "Col 72 is statistically significantly different at alpha=0.1, pval=5.437490388876791e-05\n",
      "Col 73 is statistically significantly different at alpha=0.1, pval=0.0009485161276829711\n",
      "Col 74 is statistically significantly different at alpha=0.1, pval=2.3304069644083427e-05\n",
      "Col 75 is statistically significantly different at alpha=0.1, pval=0.0013141362654415952\n",
      "Col 76 is statistically significantly different at alpha=0.1, pval=0.0041556366610725395\n",
      "Col 77 is statistically significantly different at alpha=0.1, pval=0.049254322460874404\n",
      "Col 78 is statistically significantly different at alpha=0.1, pval=0.0012201116479029059\n",
      "Col 79 is statistically significantly different at alpha=0.1, pval=0.011447923110954062\n",
      "Col 80 is statistically significantly different at alpha=0.1, pval=2.7545666977274566e-05\n",
      "Col 81 is statistically significantly different at alpha=0.1, pval=0.004855649361034896\n",
      "Col 82 is statistically significantly different at alpha=0.1, pval=0.0009722419648402623\n",
      "Col 83 is statistically significantly different at alpha=0.1, pval=2.2228171208167994e-06\n",
      "Col 84 is statistically significantly different at alpha=0.1, pval=0.0010931193465286565\n",
      "Col 85 is statistically significantly different at alpha=0.1, pval=0.003708118355227611\n",
      "Col 86 is statistically significantly different at alpha=0.1, pval=0.0012025396989430554\n",
      "Col 87 is statistically significantly different at alpha=0.1, pval=0.002927043534264323\n",
      "Col 88 is statistically significantly different at alpha=0.1, pval=0.0003203833566202527\n",
      "Col 89 is statistically significantly different at alpha=0.1, pval=0.0014535095348079373\n",
      "Col 90 is statistically significantly different at alpha=0.1, pval=0.0008812937508140129\n",
      "Col 91 is statistically significantly different at alpha=0.1, pval=0.0004775821547669485\n",
      "Col 92 is statistically significantly different at alpha=0.1, pval=0.00011922541878138975\n",
      "Col 93 is statistically significantly different at alpha=0.1, pval=0.0005134195654220438\n",
      "Col 94 is statistically significantly different at alpha=0.1, pval=1.5157130300614773e-07\n",
      "Col 95 is statistically significantly different at alpha=0.1, pval=7.296658816634491e-06\n",
      "Col 96 is statistically significantly different at alpha=0.1, pval=0.0052862745442171745\n",
      "Col 97 is statistically significantly different at alpha=0.1, pval=0.04001342019099814\n",
      "Col 98 is statistically significantly different at alpha=0.1, pval=0.0011851239539327264\n",
      "Col 99 is statistically significantly different at alpha=0.1, pval=0.00037317804048897903\n",
      "Total number diferrent was 100, which is 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Asi que hay que tener esto en mente cuando se hagas statistical tests como t-test que tiene un p-value\n",
    "# Enteder que p-value no es magico y tiene un umbral para poder informar los resultados y tratar de \n",
    "# responder la hipotesis. Cual es un umbral razonable? eso depende de la pregunta y se necesita tener\n",
    "# expertos en el dominio del tema para entender mejor qué se considera significativo\n",
    "\n",
    "# Por diversion, vamos a crear un segundo DataFrame usando una distribucion no normal. Como chi squared\n",
    "\n",
    "df2 = pd.DataFrame([np.random.chisquare(df=1, size= 100) for x in range(100)])\n",
    "test_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui vemos que todas o la mayoria de columnas para el test son estadisticamente significantes al 10%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui discutimos un poco sobre basics of hypothesis testing in python. Se introdujo la libreria Scipy con la que pueden hacerse t-test. Hay mucho que aprender sobre hypothesis testing, por ejempo, hay diferentes test que se pueden aplicar dependiendo de la forma de la data y los diferentes caminos para reportar resultados, no solamente p-values, como confidence intervals, bayesian analyses. Pero esto dara una idea basica de como empezar a comparar dos poblaciones para ver sus diferencias, lo cual es una tarea comun en la ciencia de Datos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
